<html>
<head>
<title>Syzygy: Introduction to the Phleet Distributed OS</title>
</head>
<body bgcolor="#ffffff">
<a href="index.html">Documentation Index</a>

<p><font size=+2>Syzygy: Introduction to the Phleet Distributed OS</font>

<p>This chapter explains Phleet's basic operations and is the first you
should read when beginning to set up a Syzygy cluster. Make sure you read
the section on <a href="#Firewalls">firewalls</a> with particular care. Also,
while you can run Syzygy applications without using "virtual computers",
it is recommended that you read and understand this 
<a href="#VirtualComputer">section</a>.

<p>
<ul>
<li><A HREF="#Firewalls">A Note on Firewalls</a>
<li><A HREF="#CreatingPhleetOverview">Creating a Phleet: Overview</a>
<li><A HREF="#ComputerConfig">Creating a Phleet: Configuring Computers</a>
<li><A HREF="#RunningSzgserver">Creating a Phleet: Running szgserver and szgd</a>
<li><A HREF="#TestingPhleet">Basic Testing of Your Phleet</a>
<li><A HREF="#ManagingPhleet">Managing the Phleet</a>
<li><A HREF="#VirtualComputer">Using Virtual Computers</a>
</ul>

<A NAME="Firewalls">
<p><font size=+2>A Note on Firewalls</font>
</a>

<p>IMPORTANT: Many operating systems today will install a firewall. 
The default configuration of this software might very well keep Syzygy from 
functioning. After all, as a networked system, Syzygy expects to be able to 
communicate on particular ports. The easiest option is to simply not have a 
firewall at all. However, this will be unacceptable in some environments.

<p>We now outline the ports that must be allowed through the firewall. 
<p><ol>
<li>For Syzygy to operate in a distributed fashion, you will need to create a 
Phleet. This involves running an szgserver, which involves specifying a
port for incoming TCP connections. The computer on which 
szgserver runs must allow connections to this port.
<li>The "dhunt" and "dlogin" commands rely on UDP packets reaching port
4620 to operate properly. 
<li>Finally, every computer that is part of the Phleet will have a block of
ports used by Syzygy components to connect to one another. This
block is set by the "dports" command, defaulting to 4700-4899. This block,
however you choose to define it, must be let through your firewall (TCP).
</ol>

<A NAME="CreatingPhleetOverview">
<p><font size=+2>Creating a Phleet: Overview</font>
</a>

<p>Please make sure you read the previous section on 
<a href="#Firewalls">firewalls</a>.

<p>Syzygy depends on distributed operating system, Phleet, that helps 
programs share data with one another, stores
configuration information for users, and provides an interface for 
managing the cluster software components. A single instance of the Phleet
kernel, szgserver, runs for each distributed operating system instance. 
Different users on different computers can be connected to different
distributed operating system instances, or Phleets, dynamically changing
their associations over time. However, the most common configuration will
be a group of related computers, with a szgserver running on one of them
and providing the distributed operating system for all the users of the
cluster. We focus on setting up such a system. In what follows, we assume
that broadcast packets can travel between any computers that will be in your
Phleet.

<p>NOTE: szgserver is meant to run in the background for a long
time (like a linux daemon or windows service). In general, you'll only
need to run one copy of szgserver to manage your cluster.
Here are some common misunderstandings:
<p><ul> 
<li>Do not run a new copy of szgserver for each application.
<li>Do not run a copy of szgserver on every computer in your cluster.
</ul>

<p>The sections that follow will show you how to set up and test your Phleet
at a basic level. The most complex one deals with using the command line tools
to create the szg.conf file for each computer in the cluster. Next, you will
be walked through running szgserver and szgd's. Finally, you will see how to
automatically connect running applications, test your Phleet, and have a
fault tree with which to diagnose problems.

<a name="ComputerConfig">
<p><font size="+2">Creating a Phleet: Configuring Computers</font>
</a>

<p>This section shows you how to configure each computer in your 
cluster using Syzygy's command-line tools, in this case dname, daddinterface,
ddelinterface, and dports. This produces an szg.conf file whose location is
determined as follows:

<p><ol>
<li>If the environment variable SZG_CONF is set and has a value different
    from "NULL", it gives the DIRECTORY in which the config file is located.
    When giving this directory, DO NOT use a trailing slash.
<li>If the environment variable SZG_CONF is unset or has the value "NULL",
    the location of the config file varies depending on operating system.
    <ul>
    <li>On Unix-like operating systems (Linux, Mac OS X, Irix), the 
    default location is in /etc and the config file is
    /etc/szg.conf.
    <li>On Windows, the default location is in c:\szg and the config file
    is c:\szg\szg.conf.
    </ul>
</ol>

<p>Please note: All Phleet users MUST be able to change into the directory
containing the szg.conf file.

<p>Every Syzygy program will try to read the szg.conf config file. It must
exist for programs to work in Phleet (i.e. clustered) mode but it is not 
necessary for programs to work in <a href="Standalone.html">standalone</a> 
mode. An important design goal of Syzygy is to enable ordinary users to
try it in Phleet mode, Since a non-root user cannot write to /etc, 
this necessitates letting an environment variable optionally define the 
config file's location. Also, a Windows installation is not guaranteed to
have a c: drive, even though this is uncommon, once again requiring
flexibility in determining szg.conf's location.

<p><ol>
<li>Login to the computer to be configured.
<li>You will use the command line to alter szg.conf, whose location is
    either a default or defined via $SZG_CONF, as described at the start of
    this section. 
<li>Name the computer. This must be unique across all the computers in your
    system. A good choice is the short version of your computer's DNS name
    (i.e. XXX.YYY.YYY.edu becomes XXX) but other names are possible. We assume
    that you can write to the file's location.

<pre>
  dname computer_name
</pre>

<li>The syzygy config file contains information about the network interfaces
    in the computer. This information is used, for instance, in automatically
    connecting various components to one another. To add an interface to the 
    config file:

<pre>
  daddinterface network_name address [netmask]
</pre>

<li>The network_name gives a descriptive name for the network. Internet 
    addresses should use "internet". Private networks can use an arbitrary 
    name, but this should be consistent across the private network and 
    different from that assigned to other private networks in the distributed 
    system. For instance, the distributed system might contain 2 clusters, 
    each connected internally by a distinct 192.168.0.XXX private network. In 
    this case, the network_name associated with each should be different. This 
    lets Phleet operate properly with respect to connection brokering.
<li>The primary or default network for each computer should be added *first*.
    This network should be one to which all computers in the distributed system
    are connected. Components will expect to connect to the szgserver on this
    network. Furthermore, if components have a choice about how to connect to
    one another (i.e. they are connected by several networks), they will
    default to using the first network.
<li>You can optionally specify a netmask for the interface. By default,
    a netmask of 255.255.255.0 is used.
<li>One can remove networks from the config file.

<pre>
  ddelinterface network_name address
</pre>

<li>Syzygy operates using connection brokering. You will not explicitly assign 
    the ports on which servers will listen for connections. Instead, Phleet 
    will assign the servers ports based on an available pool it maintains 
    (on a per-computer basis). By default, the block 4700-4799 is used, which 
    is likely to be OK on both Unix and Windows machines. However, you can 
    change this using the following command:

<pre>
  dports first size
</pre>

<li>This command allocates a block of ports beginning at "first" and 
    containing "size" many ports. IMPORTANT: you'll want every port in this 
    block to be free and for user services to be able to bind to them. 
    Furthermore, the block should be of reasonable size, in order to 
    accomodate all the services that might run on the computer. All in all, it 
    is best to be generous when assigning the size, consequently the default 
    value. NOTE: some Windows versions do not like user services to bind to 
    ports 5000 and above.
<li>After having issued these commands, you should check the stored config 
    file.

<pre>
  dconfig
</pre>
  
<li>The output might look something like this:

<pre>
  Phleet Configuration:
    computer = MY_COMPUTER
    network = internet, address = XXX.XXX.XXX.XXX, netmask=255.255.255.0
    network = wall, Address = 192.168.0.1, netmask=255.255.255.0
    ports = 4700 - 4899
</pre>

<li>The dconfig command will display the information in the config file Phleet
    components running on the computer will use, given a particular value of
    $SZG_CONF.
</ol>

<a name="RunningSzgserver">
<p><font size=+2>Creating a Phleet: Running szgserver and szgd</font>

<p>In this section, we show you how to run an szgserver, the program that
controls the Phleet distributed OS, and szgd's, the Syzygy remote
execution daemons. These programs will form the backbone of your Phleet,
enabling you to control your cluster from an arbitrary location on your
network.

<ol>
<li>Choose a computer that will act as the server for the Phleet
    distributed OS. This is where the szgserver program will run. Do not
    run szgserver on more than one computer. The syntax for the szgserver
    command is as follows:

<pre>
  szgserver server_name server_port [condition_1] ... [condition_n]
</pre>

    You need to give the szgserver program a name, server_name, which
    can be anything. You also give it a server_port at which it listens
    for connections and the IP address of the server machine. DO NOT
    choose a port within the ports block that will be used for
    connection brokering on the computer running szgserver. If you
    do not specify any option "conditions", the szgserver will accept
    connections from anywhere. However, if you do specify 
    conditions, the szgserver will only accept conditions from IP
    addresses that meet one of the conditions.

<li>The following condition:

<pre>
  192.168.0.0/255.255.255.0
</pre>

    will only be matched by IP addresses that, when masked by 255.255.255.0,
    equal 192.168.0.0.
    
<li>The following szgserver will, for instance, accept a connection on 
port 4343 from 192.168.0.224 but not 10.0.0.2.

<pre>
  szgserver generic 4343 192.168.0.0/255.255.255.0
</pre>

<p>A reminder about firewalls:  if running one on the computer running
szgserver, poke holes for ports 4620/UDP and 4343/TCP
(where "4343" is the port specified on the example command line above).

<li>To interact with the system, you'll need to login to the Phleet, which
    is done on a per-machine basis.
    To run Syzygy commands on a particular machine, first login on that machine 
    by typing (and making sure that the szgserver_name is the same as used above):

<pre>
  dlogin szgserver_name syzygy_user_name
</pre>

   Conceptually, this command logs you in to the Phleet managed by the
   szgserver with szgserver_name
   using Syzygy user name syzygy_user_name. Syzygy login associates
   a user name (in the context of the computer's native OS) with a Syzygy user name.
   Subsequently (until dlogout), any Syzygy command issued on that computer by the
   given user (as that identity is understood by the OS)
   will be sent to the Phleet and executed using the given Syzygy user's identity.
<li>Make sure that you have issued the dlogin command, as above, on each computer
    that will be part of your Phleet. Upon success, a login file will
    be written, with its location depending on the value of $SZG_LOGIN and
    its name depending on the native OS's idea of the user name XXX.
    <ul>
    <li>If $SZG_LOGIN is unset or is "NULL", then, on Unix, the
    login file will be /tmp/szg_XXX.conf while on Windows the login file
    will be C:\szg\szg_XXX.conf.
    <li>Otherwise, the file's location will be given by $SZG_LOGIN, with its
    filename as above.
    </ul>
<li>Upon successful dlogin, the information in the login file will be printed,
    looking something like:
    
<pre>
  Phleet Login:
      system user name = schaeffr
      phleet user name = ben
      szgserver        = "generic", XXX.XXX.XXX.XXX:4343
</pre>

<li>Whenever you run a Syzygy program, it uses this above procedure to
find a login file directing it to a particular szgserver. Please
recall that the login file location depends on $SZG_LOGIN.

<li>We now outline the operation of dlogin. When connecting to the szgserver
    via name, it works by first reading the Phleet config file on the local
    machine (whose location can be altered by $SZG_CONF). The program then 
    sends broadcast packets on each of the networks listed there 
    (i.e. if 192.168.0.1 is an address with netmask 255.255.255.0, a packet
    is sent to 192.168.0.255). The szgserver continually listens for such packets,
    and, if it receives one tagged with its own name, returns information about 
    how to connect.
<li>This can fail in several ways. The failure possibilities and remedies are:
    <p><ul>
    <li>Broadcast packets are filtered by the network between dlogin and the szgserver.
        For instance, note that the loopback interface (127.0.0.1) filters broadcast
        packets on several systems supported by Syzygy (like Mac OS X). 
        You can work around this by connecting explicitly to the szgserver (see below).
    <li>The networks in the Phleet config file on the machine on which dlogin was issued
        are incorrect. Check them again using dconfig.
    <li>The szgserver name given in dlogin and the name of the running szgserver are 
         different. Make sure they are the same.
    <li>The szgserver tells dlogin to connect using the first address in the phleet config
        file located on the MACHINE RUNNING SZGSERVER. The machine running dlogin must
        be able to reach this IP address. If not, reorder the addresses in the phleet
        config file on the szgserver machine using ddelinterface and daddinterface
        ON THAT MACHINE.
    </ul>
<li>If broadcast packets will not travel from dlogin to szgserver, you can issue an explicit
    version of dlogin.
    
<pre>
  dlogin szgserver_IP_address szgserver_port syzygy_user_name
</pre>

<li>Now, on each machine in the distributed system, run szgd, the phleet remote execution
    daemon.
    
<pre>
  szgd
</pre>

    When you run szgd on a machine, make sure that you are logged-in (OS-wise) 
    to that machine as you were when you issued the dlogin command.
<li>Now, on any machine in the distributed system, type "dps". You'll see something like:

<pre>
  computer1/szgd/0
  computer2/szgd/1
  computer3/szgd/2
  computer4/szgd/3
  computer5/szgd/4
  computer6/szgd/5
</pre>

    There should be a line for each computer on which you have run szgd.
<li>PLEASE NOTE: the szgd's are not necessary for many Phleet functions, like messaging
or connection brokering, though they are required to operate a
<a href="#VirtualComputer">virtual computer</a>.
</ol>

<a name="TestingPhleet">
<p><font size=+2>Basic Testing of Your Phleet</font>
</a>

<p>We can now verify that basic connection brokering works between computers in your
Phleet. The BarrierServer test program synchronizes the operation of a collection
of BarrierClient test programs operating anywhere in the Phleet. The BarrierClient
programs can appear and disappear, as can the BarrierServer. When computers are added
or removed, the other components dynamically adjust. For instance, if BarrierServer
is halted, any BarrierClients will wait for a BarrierServer to start up and register
with the szgserver, at which point they will connect to it. If a new BarrierClient
starts up, it will be dynamically added to the BarrierServer's synchronization group,
with its internal loop falling into lockstep with that of the other BarrierClients.

<p><ol>
<li>Start BarrierServer on one of your Phleet nodes.
<li>Start BarrierClient on another node. If all is well, the BarrierClient will
begin printing the average time necessary to perform each synchronization through
the BarrierServer.
<li>Troubleshooting:
  <ul>
  <li>If BarrierClient fails to start, you do not dlogin.
  <li>If BarrierClient starts but does not start printing, you have entered the
      IP address incorrectly in the szg.conf file on the BarrierServer machine.
      Use dconfig to examine it and ddelinterface/daddinterface to fix it.
  </ul>
<li>Start BarrierClient on other nodes.
<li>Kill BarrierServer and restart on another node. If the various running
    BarrierClients do not reconnect, see the troubleshooting section above.
</ol>

<A NAME="ManagingPhleet">
<p><font size=+2>Managing the Phleet</font>
</a>

<p>Phleet includes commands to manage components running on the cluster, distribute user
configuration information, and to send messages from one component to another. We include
an incomplete listing of Phleet-related commands here, with descriptions of their uses.
The complete list, with explanation of parameters, is contained in
another <a href="Phleet.html">chapter</a>.

<p><ul>
<li>dps: Lists all the components registered with the Phleet.
<li>dex: Either launches a component on a single computer or an entire application
across a virtual computer.
<li>dmsg: Sends a message to a Phleet component.
<li>dkill: Sends a "quit" message to a Phleet component, which will be obeyed if the
component is well-behaved. Can also be used to forcibly connect the component from
the szgserver.
<li>dget: Retrieve a value from the Phleet parameter database associated with the
current Phleet user.
<li>dset: Set a value in the Phleet paramtere database associated with the current
Phleet user.
<li>dbatch: Enter an entire parameter file into the user's Phleet database.
</ul>

<A NAME="VirtualComputer">
<p><font size=+2>Using Virtual Computers</font>

<p>Syzygy components that need to communicate, like a graphics display (szgrender) and
a source of graphics information, can be launched by hand computers in the Phleet and
will find one another automatically via szgserver's connection brokering. You could,
for instance, launch your cluster application via shell scripts. However, the
Syzygy application frameworks (distributed scene graph and master/slave) have a built-in
understanding of "virtual computers", a way to define how your application should
launch and run on the Phleet. By using these definitions, you give Syzygy more ability
to seemlessly manage your application's life cycle. Multiple virtual computers can
operate within a single Phleet, some of which are linked by sharing a "location"
(see below) and some of which are completely independent of one another.
<p>The following is a virtual computer definition for a 6 graphics pipe display, that
includes sound, and is controlled via a simulated tracker interface (the wandsimserver
described below).

<pre>
  wall SZG_CONF virtual true
  wall SZG_CONF location wall
  wall SZG_CONF relaunch_all false
  wall SZG_TRIGGER map smoke
  wall SZG_MASTER map SZG_SCREEN0
  wall SZG_SCREEN number_screens 6
  wall SZG_SCREEN0 map wall1/SZG_SCREEN0
  wall SZG_SCREEN0 networks wall
  wall SZG_SCREEN1 map wall2/SZG_SCREEN0
  wall SZG_SCREEN1 networks wall
  wall SZG_SCREEN2 map wall3/SZG_SCREEN0
  wall SZG_SCREEN2 networks wall
  wall SZG_SCREEN3 map wall4/SZG_SCREEN0
  wall SZG_SCREEN3 networks wall
  wall SZG_SCREEN4 map wall5/SZG_SCREEN0
  wall SZG_SCREEN4 networks wall
  wall SZG_SCREEN5 map wall6/SZG_SCREEN0
  wall SZG_SCREEN5 networks wall
  wall SZG_INPUT0 map smoke/wandsimserver
  wall SZG_INPUT0 networks internet
  wall SZG_SOUND map sound
  wall SZG_SOUND networks internet
</pre>

<p>The computers in the cluster are smoke, wall1, wall2, wall3, wall4, wall5,
wall6, and sound. The name of the virtual computer is wall. By setting
SZG_CONF/virtual to true, the system understands that wall has been designated
as a virtual computer. If this value does not appear in the database, Syzygy
will not allow "wall" to be used as a virtual computer. The virtual computer name 
must not occur as the name of a physical computer in the Phleet.

<p>The "location" of the virtual computer defaults to the virtual computer's name
if SZG_CONF/location is unset. The location is used in conncetion brokering. When
a cluster application is launched on a virtual computer, it uses its location to
determine if there are running components it can reuse or that are incompatible and
must be terminated. The location is also used as a key to make sure components
running on that virtual computer only connect among themselves and not, for instance,
with components running on another virtual computer in the Phleet.

<p>Two different virtual computers can share the same set of computers happily with
respect to application launching and component reuse if they define the same location.
Different virtual computer definitions may be necessary, for instance,
when some applications require special resource configurations.

<p>By default, when an application launches on a virtual computer, it tries to
reuse compatible components, like scene graph displays (szgrender) if it is a distributed
scene graph application, sound displays (SoundRender), or sources of input information
(DeviceServer). However, sometimes a user might wish to never reuse components between
application starts. In this case, SZG_CONF/relaunch_all should be set to "true".

<p>The virtual computer has one special node that must be determined by the
user. This is the trigger node, which is set via the database value
of SZG_TRIGGER/map. When executing
an application on the virtual computer, an executable first runs on the trigger
node. This "trigger instance" scans the Phleet and determines which
running services are incompatible with the new application. These are terminated.
For instance, any application already running on the virtual computer
is forced to exit. Next, the trigger
instance begins launching needed application components. When this is done, the
trigger instance behaves differently if it belongs to a distributed scene graph
application or if it belongs to a master/slave application. In the former case,
it actually begins to run the application. In the later case, it only launches the
other application components. In either case, the 
trigger instance waits for a kill message and, when it receives such, 
shuts down its launched components.  

<p>While the trigger node has meaning for both master/slave and
distributed scene graph applications, the "master" (as determined by 
SZG_MASTER/map) does not. This designates the screen that will run the master
instance of the application for a master/slave program, which creates and distributes 
application state to the slave instances. In the example above,
the master instance will run on wall1 and will be the instance associated with
SZG_SCREEN0 (on wall1). Note that there may be more than one screen associated
to a given cluster node.

<p>The virtual computer needs several rendering screens to be defined. First,
the value of SZG_SCREEN/number_screens tells how many graphics pipes are part
of the virtual computer. For each screen, two values need to be set. One determines
where the component that produces its graphics will run. The second
determines the networks the graphics component will use to communicate. The above
example shows a virtual computer with two distinct networks, the public internet
and an internal private network designated "wall". To increase the efficiency of
the graphics data transfer, we force the graphics communication to occur via the
private network only and have the other communication (sound and input) occur
via the internet. 

<p>The user also needs to map an input device to run applications on the 
virtual computer. The value of SZG_INPUT0/map is of the format:

<pre>
  AAA(0)/BBB(0)/AAA(1)/BBB(1)/.../AAA(n)/BBB(n)
</pre>

The "AAA" entries are all computer names which designate the computers upon which
the corresponding "BBB" components will be launched. A sequence of components
must be allowed since some "virtual" input devices can require the cooperation
of several components running on multiple machines for their operation. The
first component in this list is the one that will communicate directly with the
application. The other components will funnel their data to this one.

<p>The value of SZG_INPUT0/networks determines the communications path the
input devices will use.

<p>SZG_SOUND/map gives the the computer upon which SoundRender will run.
SZG_SOUND/networks gives the communications path it will use.

<p>To launch an application on a virtual computer, you will need a copy of
szgd running on every "mapped" node (i.e. any node whose name appears as
the value of a "map" database value). Once this is done, to launch hspace on
virtual computer wall, use the command:

<pre>
  dex wall hspace
</pre>

<p>There are two ways to kill an application running on the virtual computer. You
can directly kill the trigger instance. In our case, that would entail the following:

<pre>
  dmsg -c wall quit
</pre>

<p>Or you can:

<pre>
  dkillall wall
</pre>

</body>
</html>
