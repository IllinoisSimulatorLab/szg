<html>
<head>
<title>Syzygy: Introduction to the Phleet Distributed OS</title>
</head>
<body bgcolor="#ffffff">
<a href="index.html">Documentation Index</a>

<p><font size=+2>Syzygy: Introduction to the Phleet Distributed OS</font>

<p>This chapter explains Phleet's basic operations and is the first you
should read when beginning to set up a Syzygy cluster. Make sure you read
the section on <a href="#Firewalls">firewalls</a> with particular care. If
this is your first time using Syzygy, the sections on "Creating a Phleet" will
walk you through, step by step, the basic process of setting up your
distributed system. Also,
while you can run Syzygy applications without using "virtual computers",
it is recommended that you read and understand the
<a href="#VirtualComputer">section</a> describing virtual computers.
In addition, special command line
arguments and environment variables can be used to affect the operation
of your Syzygy programs, as is explained <a href="#Context">here</a>. These
"contexts" are especially important when making your own tools (like via
a web interface) to control the Phleet. To understand how Phleet 
components cooperate (both connect to one another and avoid stepping on one
another's toes), you should read the <a href="#ServiceLocks">section</a> 
dealing with connection brokering, service names, and locks. Finally, the 
<a href="#Troubleshooting">troubleshooting</a> section includes
explanations of commonly encountered problems.

<p>
<ul>
<li><A HREF="#Firewalls">A Note on Firewalls</a>
<li><A HREF="#CreatingPhleetOverview">Creating a Phleet: Overview</a>
<li><A HREF="#ComputerConfig">Creating a Phleet: Configuring Computers</a>
<li><A HREF="#RunningSzgserver">Creating a Phleet: Running szgserver and szgd</a>
<li><A HREF="#TestingPhleet">Basic Testing of Your Phleet</a>
<li><A HREF="#ManagingPhleet">Managing the Phleet</a>
<li><A HREF="#VirtualComputer">Using Virtual Computers</a>
<li><a href="#Context">Component Contexts in Phleet</a>
<li><a href="#ServiceLocks">Connection Brokering, Service Names, and Locks</a>
<li><a href="#Troubleshooting">Troubleshooting</a>
</ul>

<A NAME="Firewalls">
<p><font size=+2>A Note on Firewalls</font>
</a>

<p>IMPORTANT: Many operating systems today will install a firewall. 
The default configuration of this software might very well keep Syzygy from 
functioning. After all, as a networked system, Syzygy expects to be able to 
communicate on particular ports. The easiest option is to simply not have a 
firewall at all. However, this will be unacceptable in some environments.

<p>We now outline the ports that must be allowed through the firewall. 
<p><ol>
<li>For Syzygy to operate in a distributed fashion, you will need to create a 
Phleet. This involves running an szgserver, which involves specifying a
port for incoming TCP connections. The computer on which 
szgserver runs must allow connections to this port.
<li>The "dhunt" and "dlogin" commands rely on UDP packets reaching port
4620 to operate properly. 
<li>Finally, every computer that is part of the Phleet will have a block of
ports used by Syzygy components to connect to one another. This
block is set by the "dports" command, defaulting to 4700-4899. This block,
however you choose to define it, must be let through your firewall (TCP).
</ol>

<A NAME="CreatingPhleetOverview">
<p><font size=+2>Creating a Phleet: Overview</font>
</a>

<p>Please make sure you read the previous section on 
<a href="#Firewalls">firewalls</a>.

<p>Syzygy depends on distributed operating system, Phleet, that helps 
programs share data with one another, stores
configuration information for users, and provides an interface for 
managing the cluster software components. A single instance of the Phleet
kernel, szgserver, runs for each distributed operating system instance. 
Different users on different computers can be connected to different
distributed operating system instances, or Phleets, dynamically changing
their associations over time. However, the most common configuration will
be a group of related computers, with a szgserver running on one of them
and providing the distributed operating system for all the users of the
cluster. We focus on setting up such a system. In what follows, we assume
that broadcast packets can travel between any computers that will be in your
Phleet.

<p>NOTE: szgserver is meant to run in the background for a long
time (like a linux daemon or windows service). In general, you'll only
need to run one copy of szgserver to manage your cluster.
Here are some common misunderstandings:
<p><ul> 
<li>Do not run a new copy of szgserver for each application.
<li>Do not run a copy of szgserver on every computer in your cluster.
</ul>

<p>The sections that follow will show you how to set up and test your Phleet
at a basic level. The most complex one deals with using the command line tools
to create the szg.conf file for each computer in the cluster. Next, you will
be walked through running szgserver and szgd's. Finally, you will see how to
automatically connect running applications, test your Phleet, and have a
fault tree with which to diagnose problems.

<a name="ComputerConfig">
<p><font size="+2">Creating a Phleet: Configuring Computers</font>
</a>

<p>This section shows you how to configure each computer in your 
cluster using Syzygy's command-line tools, in this case dname, daddinterface,
ddelinterface, and dports. This produces an szg.conf file whose location is
determined as follows:

<p><ol>
<li>If the environment variable SZG_CONF is set and has a value different
    from "NULL", it gives the DIRECTORY in which the config file is located.
    When giving this directory, DO NOT use a trailing slash.
<li>If the environment variable SZG_CONF is unset or has the value "NULL",
    the location of the config file varies depending on operating system.
    <ul>
    <li>On Unix-like operating systems (Linux, Mac OS X, Irix), the 
    default location is in /etc and the config file is
    /etc/szg.conf.
    <li>On Windows, the default location is in c:\szg and the config file
    is c:\szg\szg.conf.
    </ul>
</ol>

<p>Please note: All Phleet users MUST be able to change into the directory
containing the szg.conf file.

<p>Every Syzygy program will try to read the szg.conf config file. It must
exist for programs to work in Phleet (i.e. clustered) mode but it is not 
necessary for programs to work in <a href="Standalone.html">standalone</a> 
mode. An important design goal of Syzygy is to enable ordinary users to
try it in Phleet mode, Since a non-root user cannot write to /etc, 
this necessitates letting an environment variable optionally define the 
config file's location. Also, a Windows installation is not guaranteed to
have a c: drive, even though this is uncommon, once again requiring
flexibility in determining szg.conf's location.

<p><ol>
<li>Login to the computer to be configured.
<li>You will use the command line to alter szg.conf, whose location is
    either a default or defined via $SZG_CONF, as described at the start of
    this section. 
<li>Name the computer. This must be unique across all the computers in your
    system. A good choice is the short version of your computer's DNS name
    (i.e. XXX.YYY.YYY.edu becomes XXX) but other names are possible. We assume
    that you can write to the file's location.

<pre>
  dname computer_name
</pre>

<li>The syzygy config file contains information about the network interfaces
    in the computer. This information is used, for instance, in automatically
    connecting various components to one another. To add an interface to the 
    config file:

<pre>
  daddinterface network_name address [netmask]
</pre>

<li>The network_name gives a descriptive name for the network. Internet 
    addresses should use "internet". Private networks can use an arbitrary 
    name, but this should be consistent across the private network and 
    different from that assigned to other private networks in the distributed 
    system. For instance, the distributed system might contain 2 clusters, 
    each connected internally by a distinct 192.168.0.XXX private network. In 
    this case, the network_name associated with each should be different. This 
    lets Phleet operate properly with respect to connection brokering.
<li>The primary or default network for each computer should be added *first*.
    This network should be one to which all computers in the distributed system
    are connected. Components will expect to connect to the szgserver on this
    network. Furthermore, if components have a choice about how to connect to
    one another (i.e. they are connected by several networks), they will
    default to using the first network.
<li>You can optionally specify a netmask for the interface. By default,
    a netmask of 255.255.255.0 is used.
<li>One can remove networks from the config file.

<pre>
  ddelinterface network_name address
</pre>

<li>Syzygy operates using connection brokering. You will not explicitly assign 
    the ports on which servers will listen for connections. Instead, Phleet 
    will assign the servers ports based on an available pool it maintains 
    (on a per-computer basis). By default, the block 4700-4799 is used, which 
    is likely to be OK on both Unix and Windows machines. However, you can 
    change this using the following command:

<pre>
  dports first size
</pre>

<li>This command allocates a block of ports beginning at "first" and 
    containing "size" many ports. IMPORTANT: you'll want every port in this 
    block to be free and for user services to be able to bind to them. 
    Furthermore, the block should be of reasonable size, in order to 
    accomodate all the services that might run on the computer. All in all, it 
    is best to be generous when assigning the size, consequently the default 
    value. NOTE: some Windows versions do not like user services to bind to 
    ports 5000 and above.
<li>After having issued these commands, you should check the stored config 
    file.

<pre>
  dconfig
</pre>
  
<li>The output might look something like this:

<pre>
  Phleet Configuration:
    computer = MY_COMPUTER
    network = internet, address = XXX.XXX.XXX.XXX, netmask=255.255.255.0
    network = wall, Address = 192.168.0.1, netmask=255.255.255.0
    ports = 4700 - 4899
</pre>

<li>The dconfig command will display the information in the config file Phleet
    components running on the computer will use, given a particular value of
    $SZG_CONF.
</ol>

<a name="RunningSzgserver">
<p><font size=+2>Creating a Phleet: Running szgserver and szgd</font>

<p>In this section, we show you how to run an szgserver, the program that
controls the Phleet distributed OS, and szgd's, the Syzygy remote
execution daemons. These programs will form the backbone of your Phleet,
enabling you to control your cluster from an arbitrary location on your
network.

<ol>
<li>Choose a computer that will act as the server for the Phleet
    distributed OS. This is where the szgserver program will run. Do not
    run szgserver on more than one computer. The syntax for the szgserver
    command is as follows:

<pre>
  szgserver server_name server_port [condition_1] ... [condition_n]
</pre>

    You need to give the szgserver program a name, server_name, which
    can be anything. You also give it a server_port at which it listens
    for connections and the IP address of the server machine. DO NOT
    choose a port within the ports block that will be used for
    connection brokering on the computer running szgserver. If you
    do not specify any option "conditions", the szgserver will accept
    connections from anywhere. However, if you do specify 
    conditions, the szgserver will only accept conditions from IP
    addresses that meet one of the conditions.

<li>The following condition:

<pre>
  192.168.0.0/255.255.255.0
</pre>

    will only be matched by IP addresses that, when masked by 255.255.255.0,
    equal 192.168.0.0.
    
<li>The following szgserver will, for instance, accept a connection on 
port 4343 from 192.168.0.224 but not 10.0.0.2.

<pre>
  szgserver generic 4343 192.168.0.0/255.255.255.0
</pre>

<p>A reminder about firewalls:  if running one on the computer running
szgserver, poke holes for ports 4620/UDP and 4343/TCP
(where "4343" is the port specified on the example command line above).

<li>To interact with the system, you'll need to login to the Phleet, which
    is done on a per-machine basis.
    To run Syzygy commands on a particular machine, first login on that machine 
    by typing (and making sure that the szgserver_name is the same as used above):

<pre>
  dlogin szgserver_name syzygy_user_name
</pre>

   Conceptually, this command logs you in to the Phleet managed by the
   szgserver with szgserver_name
   using Syzygy user name syzygy_user_name. Syzygy login associates
   a user name (in the context of the computer's native OS) with a Syzygy user name.
   Subsequently (until dlogout), any Syzygy command issued on that computer by the
   given user (as that identity is understood by the OS)
   will be sent to the Phleet and executed using the given Syzygy user's identity.
<li>Make sure that you have issued the dlogin command, as above, on each computer
    that will be part of your Phleet. Upon success, a login file will
    be written, with its location depending on the value of $SZG_LOGIN and
    its name depending on the native OS's idea of the user name XXX.
    <ul>
    <li>If $SZG_LOGIN is unset or is "NULL", then, on Unix, the
    login file will be /tmp/szg_XXX.conf while on Windows the login file
    will be C:\szg\szg_XXX.conf.
    <li>Otherwise, the file's location will be given by $SZG_LOGIN, with its
    filename as above.
    </ul>
<li>Upon successful dlogin, the information in the login file will be printed,
    looking something like:
    
<pre>
  Phleet Login:
      system user name = schaeffr
      phleet user name = ben
      szgserver        = "generic", XXX.XXX.XXX.XXX:4343
</pre>

<li>Whenever you run a Syzygy program, it uses this above procedure to
find a login file directing it to a particular szgserver. Please
recall that the login file location depends on $SZG_LOGIN.

<li>We now outline the operation of dlogin. When connecting to the szgserver
    via name, it works by first reading the Phleet config file on the local
    machine (whose location can be altered by $SZG_CONF). The program then 
    sends broadcast packets on each of the networks listed there 
    (i.e. if 192.168.0.1 is an address with netmask 255.255.255.0, a packet
    is sent to 192.168.0.255). The szgserver continually listens for such packets,
    and, if it receives one tagged with its own name, returns information about 
    how to connect.
<li>This can fail in several ways. The failure possibilities and remedies are:
    <p><ul>
    <li>Broadcast packets are filtered by the network between dlogin and the szgserver.
        For instance, note that the loopback interface (127.0.0.1) filters broadcast
        packets on several systems supported by Syzygy (like Mac OS X). 
        You can work around this by connecting explicitly to the szgserver (see below).
    <li>The networks in the Phleet config file on the machine on which dlogin was issued
        are incorrect. Check them again using dconfig.
    <li>The szgserver name given in dlogin and the name of the running szgserver are 
         different. Make sure they are the same.
    <li>The szgserver tells dlogin to connect using the first address in the phleet config
        file located on the MACHINE RUNNING SZGSERVER. The machine running dlogin must
        be able to reach this IP address. If not, reorder the addresses in the phleet
        config file on the szgserver machine using ddelinterface and daddinterface
        ON THAT MACHINE.
    </ul>
<li>If broadcast packets will not travel from dlogin to szgserver, you can issue an explicit
    version of dlogin.
    
<pre>
  dlogin szgserver_IP_address szgserver_port syzygy_user_name
</pre>

<li>Now, on each machine in the distributed system, run szgd, the phleet remote execution
    daemon.
    
<pre>
  szgd
</pre>

    When you run szgd on a machine, make sure that you are logged-in (OS-wise) 
    to that machine as you were when you issued the dlogin command.
<li>Now, on any machine in the distributed system, type "dps". You'll see something like:

<pre>
  computer1/szgd/0
  computer2/szgd/1
  computer3/szgd/2
  computer4/szgd/3
  computer5/szgd/4
  computer6/szgd/5
</pre>

    There should be a line for each computer on which you have run szgd.
<li>PLEASE NOTE: the szgd's are not necessary for many Phleet functions, like messaging
or connection brokering, though they are required to operate a
<a href="#VirtualComputer">virtual computer</a>.
</ol>

<a name="TestingPhleet">
<p><font size=+2>Basic Testing of Your Phleet</font>
</a>

<p>We can now verify that basic connection brokering works between computers in
your Phleet. The BarrierServer test program synchronizes the operation of a 
collection of BarrierClient test programs operating anywhere in the Phleet. The
BarrierClient programs can appear and disappear, as can the BarrierServer. 
When computers are added or removed, the other components dynamically adjust. 
For instance, if BarrierServer is halted, any BarrierClients will wait for a 
BarrierServer to start up and register with the szgserver, at which point they 
will connect to it. If a new BarrierClient starts up, it will be dynamically 
added to the BarrierServer's synchronization group, with its internal loop 
falling into lockstep with that of the other BarrierClients.

<p><ol>
<li>Start BarrierServer on one of your Phleet nodes.
<li>Start BarrierClient on another node. If all is well, the BarrierClient will
begin printing the average time necessary to perform each synchronization 
through the BarrierServer.
<li>Troubleshooting:
  <ul>
  <li>If BarrierClient fails to start, make sure dlogin succeeded on the
      machine on which it is running..
  <li>If BarrierClient starts but does not start printing, you have entered the
      IP address incorrectly in the szg.conf file on the BarrierServer machine.
      Use dconfig to examine it and ddelinterface/daddinterface to fix it.
  </ul>
<li>Start BarrierClient on other nodes.
<li>Kill BarrierServer and restart on another node. If the various running
    BarrierClients do not reconnect, see the troubleshooting section above.
</ol>

<A NAME="ManagingPhleet">
<p><font size=+2>Managing the Phleet</font>
</a>

<p>Phleet includes commands to manage components running on the cluster,
distribute user configuration information, and to send messages from one 
component to another. We include an incomplete listing of Phleet-related 
commands here, with descriptions of their uses. The complete list, with 
explanation of parameters, is contained in another 
<a href="Phleet.html">chapter</a>.

<p><ul>
<li>dps: Lists all the components registered with the Phleet.
<li>dex: Either launches a component on a single computer or an entire 
application across a virtual computer.
<li>dmsg: Sends a message to a Phleet component.
<li>dkill: Sends a "quit" message to a Phleet component, which will be obeyed 
if the component is well-behaved. Can also be used to forcibly connect the 
component from the szgserver.
<li>dget: Retrieve a value from the Phleet parameter database associated with 
the current Phleet user.
<li>dset: Set a value in the Phleet paramtere database associated with the 
current Phleet user.
<li>dbatch: Enter an entire parameter file into the user's Phleet database.
</ul>

<A NAME="VirtualComputer">
<p><font size=+2>Using Virtual Computers</font>

<p>Syzygy components that need to communicate, like a graphics display 
(szgrender) and a source of graphics information (like a VR application), can 
be launched by hand computers in the Phleet and will find one another 
automatically via szgserver's connection brokering. You could, for instance, 
launch your cluster application via shell scripts. However, the Syzygy 
application frameworks (distributed scene graph and master/slave) have a 
built-in understanding of "virtual computers", a way to define how your 
application should launch and run on the Phleet. By using these definitions, 
you give Syzygy more ability to seemlessly manage your application's life 
cycle. Multiple virtual computers can operate within a single Phleet, some of 
which are linked by sharing a "location" (see below) and some of which are 
completely independent of one another.

<p>The following is a virtual computer definition for a 6 screen graphics 
display, that includes sound, and is controlled via a simulated tracker 
interface (the inputsimulator described <a href="TrackingSim.html">here</a>).

<p>This definition must be entered into the Phleet parameter database via
dset or dbatch. Please see the chapter on 
<a href="ConfigurationExamples.html">configuration</a> for details on how
to do this.

<pre>
  wall SZG_CONF virtual true
  wall SZG_CONF location wall
  wall SZG_CONF relaunch_all false
  wall SZG_TRIGGER map smoke
  wall SZG_MASTER map SZG_DISPLAY0
  wall SZG_DISPLAY number_screens 6
  wall SZG_DISPLAY0 map wall1/SZG_DISPLAY0
  wall SZG_DISPLAY0 networks wall
  wall SZG_DISPLAY1 map wall2/SZG_DISPLAY0
  wall SZG_DISPLAY1 networks wall
  wall SZG_DISPLAY2 map wall3/SZG_DISPLAY0
  wall SZG_DISPLAY2 networks wall
  wall SZG_DISPLAY3 map wall4/SZG_DISPLAY0
  wall SZG_DISPLAY3 networks wall
  wall SZG_DISPLAY4 map wall5/SZG_DISPLAY0
  wall SZG_DISPLAY4 networks wall
  wall SZG_DISPLAY5 map wall6/SZG_DISPLAY0
  wall SZG_DISPLAY5 networks wall
  wall SZG_INPUT0 map smoke/inputsimulator
  wall SZG_INPUT0 networks internet
  wall SZG_SOUND map sound
  wall SZG_SOUND networks internet
</pre>

<p>The computers in the cluster are smoke, wall1, wall2, wall3, wall4, wall5,
wall6, and sound. The name of the virtual computer is wall. By setting
SZG_CONF/virtual to true, the system understands that wall has been designated
as a virtual computer, and if this value does not appear in the database, 
Phleet will not allow "wall" to be used as a virtual computer. The virtual 
computer name must not occur as the name of a physical computer in the Phleet.

<p>The "location" of the virtual computer defaults to the virtual computer's 
name if SZG_CONF/location is unset and is used in conncetion 
brokering. When a cluster application is launched on a virtual computer, it 
uses its location to determine if there are running components it can reuse or 
that are incompatible and must be terminated. The location is used as a 
key to make sure components running on that virtual computer only connect 
among themselves and not, for instance, with components running on another 
virtual computer (with a different location) in the Phleet.

<p>Consequently, two different virtual computers can share the same set of 
computers happily with respect to application launching and component reuse if 
they use the same location. Different virtual computer definitions overlaid
on shared resources may be necessary, for instance, when some applications 
require special configurations that would be detrimental to other applications.
For instance, a psychology experiment might want an extra graphics display
in addition to those dedicated to the subject. The experimenter might sit
at the extra display and monitor the subject's progress. While an extra display
makes sense for this particular set-up, a data visualization application
would not need the extra display and might even achieve lower performance 
including it.

<p>By default, when an application launches on a virtual computer, it tries to
reuse compatible components, like scene graph displays (szgrender) if it is a 
distributed scene graph application, sound displays (SoundRender), or sources 
of input information (DeviceServer). However, sometimes a user might wish to 
never reuse components between application starts. In this case, 
SZG_CONF/relaunch_all should be set to "true".

<p>The virtual computer has one special node that must be determined by the
user. This is the trigger node, which is set via the database value
of SZG_TRIGGER/map. When on the virtual computer, an executable first runs on
the trigger node. This "trigger instance" scans the Phleet and determines which
running services are incompatible with the new application. These are 
terminated. For instance, any application already running on the virtual 
computer is forced to exit. Next, the trigger
instance begins launching needed application components. When this is done, the
trigger instance behaves differently if it belongs to a distributed scene graph
application or if it belongs to a master/slave application. In the former case,
it actually begins to run the application. In the later case, it only launches 
the other application components. In either case, the  trigger instance waits 
for a kill message and, when it receives such, shuts down its launched 
components.  

<p>While the trigger node has meaning for both master/slave and
distributed scene graph applications, the "master" (as determined by 
SZG_MASTER/map) does not. This designates the screen that will run the master
instance of the application for a master/slave program, which creates and 
distributes application state to the slave instances. In the example above,
the master instance will run on wall1 and will be the instance associated with
SZG_DISPLAY0 (on wall1). Note that there may be more than one screen associated
to a given cluster node.

<p>The virtual computer nees to define several rendering screens. First,
the value of SZG_DISPLAY/number_screens tells how many graphics pipes are part
of the virtual computer. For each screen, two values need to be set. 
One determines where the component that produces its graphics will run. 
The second determines the networks the graphics component will use to 
communicate. The above example shows a virtual computer with two distinct 
networks, the public internet and an internal private network designated 
"wall". To increase the efficiency of the graphics data transfer, we force the 
graphics communication to occur via the private network only and have the other
communication (sound and input) occur via the internet. 

<p>The user also needs to map an input device to run applications on the 
virtual computer. The value of SZG_INPUT0/map is of the format:

<pre>
  AAA(0)/BBB(0)/AAA(1)/BBB(1)/.../AAA(n)/BBB(n)
</pre>

The "AAA" entries are all computer names designating the computers where the
corresponding input event gathering components described by BBB will 
be launched. If BBB is "inputsimulator", an instance of that program will be
launched. Otherwise, an instance of DeviceServer will be launched and
initialized using the configuration information stored in the global Phleet
database parameter BBB.   

<p>A sequence of components
must be allowed since some "virtual" input devices require the cooperation
of several components running on multiple machines for their operation. The
first component in this list is the one that will communicate directly with the
application. The other components will funnel their data to this one.

<p>The value of SZG_INPUT0/networks determines the communications path the
input devices will use.

<p>SZG_SOUND/map gives the the computer upon which SoundRender will run.
SZG_SOUND/networks gives the communications path it will use.

<p>To launch an application on a virtual computer, you will need a copy of
szgd running on every "mapped" node (i.e. any node whose name appears as
the value of a "map" database value). Once this is done, to launch hspace 
(a sample application included with the szg source) on
virtual computer wall, use the command:

<pre>
  dex wall hspace
</pre>

<p>There are two ways to kill an application running on the virtual computer. 
You can directly kill the trigger instance. In our case, that would entail the 
following:

<pre>
  dmsg -c wall quit
</pre>

<p>Or you can:

<pre>
  dkillall wall
</pre>

<p>Having explained the concept of a virtual computer and how to use one,
we now go on to examine more in depth what happens when an application launches
on one, paying particular attention to the how it interacts with a previously
running application, if any. For instance, it is possible to run one
application on a virtual computer and then run another, without explicitly
killing the first. The launch process for the second application takes care
of that detail. Again, consider the command:

<pre>
  dex wall hspace
</pre>

<p>The dex command first checks to see if "wall" is the definition of
a virtual computer by seeing if wall/SZG_CONF/virtual is set to "true".
If so, dex determines the trigger of the virtual computer via
wall/SZG_TRIGGER/map and executes hspace there, passing the executable a
special parameter indicating that it is being used as a trigger to launch the 
full cluster application.

<p>When the application is being used as a launcher, it first makes
sure that a previous application isn't already running, by checking, in this
case, if the named lock "wall/SZG_DEMO/app" is held. Here, the first "wall"
appearing in the lock name indicates the virtual computer location, which
just happens in this case to be equal to the virtual computer name. If the
lock is held, the trigger sends a kill message to the lock's owner and waits 
for it to exit. Once the old application has died, the new application
enters a clean-up phase which makes sure appropriate services are
running on the cluster and incompatible ones are killed.

<p>First, it checks to see if an incompatible render program is running
on any of the screens. For virtual computer "wall" and screen 0, this
is done by checking whether anyone is currently holding the lock
wall1/SZG_DISPLAY0. Note that a render program can
remain active on a screen after an application has been killed. For
instance, szgrender stays up after a distributed scene graph
application has died since it can just accept a new connection from a
new distributed scene graph application. If there is a render program
currently registered and it is incompatible with the operation of the
new render program (szgrender, for instance, cannot display the graphics for
a master/slave program or vice-versa), the old one is killed.

<p>Next, the application launcher makes sure other required services
are running on the cluster, including input devices and sound drivers. 
In the case of virtual computer "wall", this means making sure
inputsimulator is running on computer "smoke" and that SoundRender is
running on computer "sound". In each case, if the component is running
already, leave it alone. If it isn't running, go ahead and launch it.

<p>Now that its environment has been conditioned, the trigger goes ahead and 
starts the application itself. In the
master/slave application case, it launches an application instance for each
graphics screen. In the case of a distributed scene graph
application, it goes ahead and makes sure szgrender is running on each
render node and begins executing the application code locally.

<p>Finally, the trigger waits for a kill signal, as might come from a new
application, and performs a shutdown procedure upon receiving it. In the
distributed scene graph case, this simply means shutting down itself, while,
in the master/slave case, this means telling the connected slaves to shut down
and waiting for them to do so.

<p>Here are several useful commands for managing a virtual computer:

<pre>
  dkillall virtual_computer_location
   
   Kill any application currently running in the given virtual computer
   location, along with any services like szgrender, SoundRender, or
   DeviceServer.

  dmsg -c virtual_computer_location quit

   Kills the application, if any, currently running in the given virtual
   computer location, but leaves services alone.

  restarttracker virtual_computer

   Restart services associated with virtual_computer (for instance
   input devices and sound).

  setdemomode virtual_computer [true, false]

   If second argument is "true", this sets each render node to use a fixed
   head position rather than that reported by the tracking device. The head 
   position is read from the screen config XML on each render computer. 
   Each rendering node will assume that the direction of gaze is perpendicular 
   to its screen, and the up direction of the head will be screen's up 
   direction rotated by the angle (in degrees) given by the screen's
   fixedheadupangle element. To set things back to normal mode, use "false" 
   as the second argument.

  setstereo virtual_computer [true, false]

   Turns active stereo rendering on and off for each rendering node of the 
   cluster.

  screensaver virtual_computer

   Start szgrender programs on each render node in the cluster. Since 
   szgrender is black when no application is connected, this is effectively a 
   screensaver.

  calibrationdemo virtual_computer

   Display a calibration screen useful for alignment and color matching,
   looking for the calibration picture cubecal.ppm in the SZG_DATA/path of
   each render node.
</pre>

<a name="Context">
<p><font size=+2>Component Contexts in Phleet</font>
</a>

<p>Every component in Syzygy is executed in a "context". The context is
determined via the value of the environment variable SZGCONTEXT and by
special Phleet args that can be passed to the executable. The context
determines the overall behavior of the component, whether the component
is executing as part of a virtual computer, and what networks it will use
for various sorts of communication. It consists of a list of 
variable/value pairs of the following:

<pre>
  virtual=virtual_computer_name

   The name of the virtual computer on which this component is
   executing. Set to the value "NULL" if it is not executing
   on a virtual computer (the default).

  mode/default=[trigger, master, component]

   The overall behavior of the component. The "trigger" value
   causes a syzygy application to run as a trigger instance,
   launching other necessary components on a virtual computer.
   The "master" value causes a master/slave component to
   take the master role. The "component" value is a default
   and has no effect.

  mode/graphics=SZG_DISPLAY(n)

   One of SZG_DISPLAY0, SZG_DISPLAY1, etc. If the component displays
   a graphics window, this determines the screen configuration
   it will use.

  networks/default=network_list
   
   The value network_list is a slash-delimited list of network names,
   like internet/wall. It indicates, in descending order of preference,
   the network path via which service connections should occur, 
   assuming that the service type (input, graphics, sound) has
   not already defined similar information.

  networks/input=network_list

   Defines the network path upon which input-type service connections
   should occur.

  networks/graphics=network_list

   Defines the network path upon which graphics-type service
   connections should occur.

  networks/sound=network_list

   Defines the network path upon which sound-type service connections
   should occur.

  parameter_file=parameter_file_name

   If the application is running in standalone mode (i.e. no distribution),
   it reads its parameter database from a file in its current working
   directory. The default file name is szg_parameters.txt, but this can
   be changed by passing this special parameter. 

  user=syzygy_user_name

    The user under whose auspices the syzygy application will run.

  server=IPaddress/port

    Identifies the location of the szgserver to which the component should
    attempt to connect.
</pre> 

<p>The value of SZGCONTEXT consists of a sequence of the above pairs seperated
by ';'. You can set this environment variable manually to control the behavior
of Phleet components you run locally. A sample one might look something like:

<pre>
  virtual=wall;mode/graphics=SZG_DISPLAY0;networks/graphics=wall
</pre>

<p>When launching an application on a virtual computer,

<pre>
  dex virtual_computer_name application_name
</pre>

the application launching process will set the context appropriately for
each launched component. However, sometimes the context must be set manually.
For instance, if the user wishes to restart an individual component of
the distributed application. 

<p>This can be done by setting SZGCONTEXT,
as described above. However, the user can also set the context of well-behaved
Phleet components by passing special Phleet
args to the executable. A well-behaved Phleet component will wait to check its
args until after its arSZGClient has had a chance to read these args and strip
them from argc/argv. This occurs in arSZGClient::init(...), which is called,
for instance, in arDistSceneGraphFramework::init(...) and 
arMasterSlaveFramework::init(...). If the component waits to examine its args
until after this time, the special Phleet args can be mixed freely with
other command line args. They consist of "-szg" followed by one of the
variable/value context pairs listed above. There can be any number of them.
For instance:

<pre>
  syzygy_executable -szg virtual=wall -szg networks/graphics=internet
</pre>

<p>There is one special case when the special Phleet args are not fully stripped.
The dex command should be able to pass a subset of the special Phleet args on to
its target szgd. For instance, if an szgrender is to be launched remotely and
connect to a distributed scene graph application running on a virtual computer,
we must be able to issue the follow command and have the szgrender launched on
wall1 read the "virtual=wall" arg.

<pre>
  dex wall1 szgrender -szg virtual=wall
</pre>

<p>This is a way in which the special Phleet args are more powerful than the
SZGCONTEXT variable, since they can be passed along in this special case,
whereas SZGCONTEXT is only ever evaluated locally. However, SZGCONTEXT has
the advantage not depending for its proper operation on a Phleet component
being well-behaved.

<p>On the other hand, dex, along with every other Syzygy component, will strip
and interpret the "user" and "server" args. This is useful if you want to build
a command-line based interface that can launch application on various Phleets
under various user identities, without needing dconfig or dlogin to be run
on the host machine. This is ideal for a web-based application launcher,
for instance. Note the example:

<pre>
  dex my_virtual_computer my_app -szg server=the_IP_address/the_port -szg user=the_user_name
</pre> 

<a name="ServiceLocks">
<p><font size="+2">Connection Brokering, Service Names, and Locks</font>
</a>

<p>Services in the cluster application (like sound information, scene graph
information, input events) are brokered via a service name. Components
will fail to launch if they want to provide a service with a given name but
cannot since another component already provides that service.
Consequently, it is important to understand how services get their names and
how to find out what services are being offered.

<p>Service names are parially determined by the nature of the service and
partially by the circumstances under which a component is launched. Base
service names for the various syzygy service are mentioned below.

<pre>
  SZG_INPUT(n)

   One of SZG_INPUT0, SZG_INPUT1, etc. The number refers to the driver slot.
   inputsimulator and DeviceServer offer this service. DeviceClient
   and Syzygy applications in general wish to connect to it.

  SZG_SOUND

   Source of sound information. SoundRender wishes to receive this service.
   Syzygy VR framework applications offer it.

  SZG_SOUND_BARRIER

   Synchronization for the SZG_SOUND service. SoundRender connects to this
   service. Syzygy VR framework applications offer it.

  SZG_GEOMETRY

   Source of scene graph information. szgrender wishes to receive this
   service. Distributed scene graph applications offer it.

  SZG_GEOMETRY_BARRIER

   Synchronization for the SZG_GEOMETRY service. szgrender wishes to
   receive this service. Distributed scene graph applications offer it.

  SZG_MASTER_(application_name)

   The master instance of a master/slave application offers data to the
   slaves via this service. Slave instances try to connect to this service.

  SZG_MASTER_(application_name)_BARRIER

   Synchronization for the above service.
</pre>

<p>If a user launches a component, but does not do so in the context of a 
virtual computer, the component's service name will be:

<pre>
  basic_service_name/syzygy_user_name
</pre>

This prevents the actions of one syzygy user from interfering with those
of another.

<p>On the other hand, if a user launches a component in the context of
a virtual computer, the component's service name will be:

<pre>
  virtual_computer_location/basic_service_name
</pre>

This allows various users to share application components running in the
context of a particular virtual computer location, allowing them to be used as
a communal resource.

<p>Use the "dservices" command to see a listing of all currently offered 
services. Conversely, use the "dpending" command to see a listing of all 
service requests that have yet to be matched with a service. The user can 
debug problems with the distributed system, particularly issues with one 
component not connecting to another, via these commands.

<p>To get an idea of how this all fits together, consider a brief example.
When running a distributed scene graph application, it will offer the
SZG_GEOMETRY service. A instance of szgrender wants to connect to the
SZG_GEOMETRY service, and the connection broker, seeing that such a
service is running, will send the IP/port of the service to szgrender, which
will then connect directly to the distributed scene graph application. 
In addition, the distributed scene graph application wants to connect to
the SZG_INPUT0 service, which can be provided by a DeviceServer or 
inputsimulator.

<p>Services are not the only primitive utilized by the Phleet OS. Distributed
locks can be used to ensure that sequences of Phleet operations occur 
atomically. As an example, when an application trigger begins preparing a
virtual computer for the full application's operation it obtains a named Phleet
lock, virtual_computer_location/SZG_DEMO/lock, and holds the lock until it has
finished launching its components. This ensures that an application
launching on a particular virtual computer will be able to complete the
necessary reorganization without being interrupted. 

<p>Phleet locks are also used
to reserve resources. A copy of szgd ensures that no other copy is running
on its computer by, upon start, trying to grab a computer_name/szgd lock.
If it fails, another copy must be running and it exits. If it succeeds,
it holds the lock while running. Other resource reservation locks are
SoundRender's computer_name/SoundRender lock and szgrender's
computer_name/SZG_DISPLAY(n) lock, which refers to the virtual screen that
displays its window. A final resource lock is the virtual
computer's application lock (virtual_computer_location/SZG_DEMO/app), which
ensures that only one Syzygy VR framework application is running in a given 
virtual computer location at a time.

<p>Finally, locks facilitate automatic shutdown of previously running
applications. Specifically, if a new application wants to get a resource,
it can try to get the associated lock. If it fails, the system passes
it the phleet ID of the component holding the lock. The new application
can then send the currently running component a kill message and 
wait until the lock has been released.

<p>The "dlocks" command shows a list of currently existing locks.

<a name="Troubleshooting">
<p><font size="+2">Troubleshooting</font>
</a>

<p>Phleet's connection brokering and named locks are used to simplify 
cluster set-up and management, providing a layer of abstraction that eliminates
the need to enter IP addresses and port numbers in config files and allows
components to be flexibly placed on different computers without manual
reconfiguration. While this convenience is not so important for a static
set-up which is used in essentially one way all the time, it is critical for
easy, ad-hoc clustering in a fluid lab situation where most application testing
and development occurs. It also facilitates special purpose clustering, where
particular applications (or categories of applications) need cluster
arrangements unique to them but still need to cooperate with other application
categories.

<p>Consequently, when things go wrong, like an application or application
component does not launch, or when two components refuse to connect, the user
needs to understand connection brokering and locks, along with the tools used
to diagnose problems connected to their use. Misconfiguration or misuse of
these features is the main cause of problems when using Syzygy applications.

<p><ul>
<li>szgrender, SoundRender, szgd refuse to launch: Each of these components
holds a Phleet lock corresponding to a real world resource during its 
operation. For szgrender, it is a given chunk of screen real estate. For
SoundRender, it is the sound card. For szgd, it is the computer itself. 
In the case of szgrender, executing a second copy to display on
the same virtual screen (SZG_DISPLAY0, SZG_DISPLAY1, etc.) as an already
running szgrender will result in the second copy quitting with
the following error message (the component ID will vary):

<pre>
  szgrender error: failed to get screen resource held by component 46228.
</pre>

This is normal, and indeed desirable, behavior, since some resources cannot
be shared.

<li>A Syzygy VR framework application fails to launch with the following
error message:

<pre>
  arSyncDataServer error: failed to register service.
</pre>

Again, this is an expected behavior. The VR framework application wants to
offer a sound data service. If it is not run as part of a virtual computer,
its service name will default to SZG_SOUND/the_user_name. Consequently,
when logged in to a given szgserver both on computer A and computer B a user
cannot, for instance, run one VR framework application on A and one VR
framework application on B (neither as part of a virtual computer)
simulatneously. The second one executed will quit, saying it cannot offer
the service. Similarly, if a VR framework application is already running on
a given virtual computer, the only way to run a new one is to kill the old one.
Note that, as explained in the section on 
<a href="#VirtualComputer">virtual computers</a>, launching a new
application using

<pre>
  dex virtual_computer my_application_name
</pre>

kills the previous application automatically. There two workarounds for this
problem if one wants to run multiple applications simultaneously: use
different Phleet user names or use different virtual computers for each 
application.

<li>Component A does not connect to Component B (but they should): First, you
should check that your szg.conf configuration files are correct on the
computers in question. This is verified as <a href="#TestingPhleet">here</a>.
If this tests succeeds, one of the following is almost certainly true:
  <ol>
  <li>Neither component is being run on a virtual computer, but they are
      being run with different user names. You might have dlogin'ed
      with different Phleet names on the different computers. In this case, you
      must be dlogin'ed with the same Phleet name on both computers.
  <li>One component is being run on a virtual computer but another is not.
      Again, this will fail. Both must be running in the same virtual computer
      location. Recall that every virtual computer has a location (that 
      defaults to the virtual computer name).
  <li>One component is being run in virtual computer location L1 while the 
      other is being run in virtual computer location L2. Again, this is
      going to fail. The virtual computer locations must be the same for
      the components to connect. 
  </ol>
You can diagnose these problems by running "dservices" to see the names
of services which are being offered and by running "dpending" to see the names
of unfulfilled service requests. You should see the full name of the offered
service, the full name of the request, and see how they fail to match.

<li>A computer running some Phleet components hard crashed/ got the power
cord yanked/ etc. Now these components are stuck in the Phleet process table,
and, furthermore, new instances (like szgd) cannot be launched. This problem
occurs because (in the case of szgd) the lock is still held internally by
the szgserver and the new instance cannot get it. The solution is to "dkill -9"
the old instance, which forcibly removes it from the process table. This will
allow you to start again.
</ul>

<hr>

</body>
</html>
